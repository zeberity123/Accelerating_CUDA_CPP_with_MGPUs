{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#65AE11;\">Considerations for Copy/Compute Overlap</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you will perform host-to-device and device-to-host memory transfers in non-default streams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#65AE11;\">Objectives</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the time you complete this section you will:\n",
    "\n",
    "* Understand how to use data chunking in service of achieving copy/compute overlap\n",
    "* Learn indexing techniques that allow flexible code capable of handling arbitrary data sizes and number of streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#65AE11;\">Instructor Presentation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please give your attention to the instructor while they present the slides.\n",
    "\n",
    "Run the following cell to load the slide deck for this section. If you wish, you can click on \"Start Slide Show\" once the slides appear to view them full-screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"640\"\n",
       "            src=\"https://view.officeapps.live.com/op/view.aspx?src=https://developer.download.nvidia.com/training/courses/C-AC-04-V1/task1/08_copy_compute-03.pptx\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f768c387ca0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(\"https://view.officeapps.live.com/op/view.aspx?src=https://developer.download.nvidia.com/training/courses/C-AC-04-V1/task1/08_copy_compute-03.pptx\", 900, 640)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#65AE11;\">Copy/Compute Overlap Example Code</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are two code examples for the techniques presented above, first for when the number of entries is evenly divided by the number of streams, and second, for when this is not so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#65AE11;\">N is Evenly Divided by Number of Streams</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```c\n",
    "// \"Simple\" version where number of entries is evenly divisible by number of streams.\n",
    "\n",
    "// Set to a ridiculously low value to clarify mechanisms of the technique.\n",
    "const uint64_t num_entries = 10;\n",
    "const uint64_t num_iters = 1UL << 10;\n",
    "\n",
    "// Allocate memory for all data entries. Make sure to pin host memory.\n",
    "cudaMallocHost(&data_cpu, sizeof(uint64_t)*num_entries);\n",
    "cudaMalloc    (&data_gpu, sizeof(uint64_t)*num_entries);\n",
    "\n",
    "// Set the number of streams.\n",
    "const uint64_t num_streams = 2;\n",
    "\n",
    "// Create an array of streams containing number of streams\n",
    "cudaStream_t streams[num_streams];\n",
    "for (uint64_t stream = 0; stream < num_streams; stream++)\n",
    "    cudaStreamCreate(&streams[stream]);\n",
    "\n",
    "// Set number of entries for each \"chunk\". Assumes `num_entries % num_streams == 0`.\n",
    "const uint64_t chunk_size = num_entries / num_streams;\n",
    "\n",
    "// For each stream, calculate indices for its chunk of full dataset and then, HtoD copy, compute, DtoH copy.\n",
    "for (uint64_t stream = 0; stream < num_streams; stream++) {\n",
    "\n",
    "    // Get start index in full dataset for this stream's work.\n",
    "    const uint64_t lower = chunk_size*stream;\n",
    "    \n",
    "    // Stream-indexed (`data+lower`) and chunk-sized HtoD copy in the non-default stream\n",
    "    // `streams[stream]`.\n",
    "    cudaMemcpyAsync(data_gpu+lower, data_cpu+lower, \n",
    "           sizeof(uint64_t)*chunk_size, cudaMemcpyHostToDevice, \n",
    "           streams[stream]);\n",
    "    \n",
    "    // Stream-indexed (`data_gpu+lower`) and chunk-sized compute in the non-default stream\n",
    "    // `streams[stream]`.\n",
    "    decrypt_gpu<<<80*32, 64, 0, streams[stream]>>>\n",
    "        (data_gpu+lower, chunk_size, num_iters);\n",
    "    \n",
    "    // Stream-indexed (`data+lower`) and chunk-sized DtoH copy in the non-default stream\n",
    "    // `streams[stream]`.\n",
    "    cudaMemcpyAsync(data_cpu+lower, data_gpu+lower, \n",
    "           sizeof(uint64_t)*chunk_size, cudaMemcpyDeviceToHost, \n",
    "           streams[stream]);\n",
    "}\n",
    "\n",
    "// Destroy streams.\n",
    "for (uint64_t stream = 0; stream < num_streams; stream++)\n",
    "    cudaStreamDestroy(streams[stream]);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#65AE11;\">N is Not Evenly Divided by Number of Streams</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```c\n",
    "// Able to handle when `num_entries % num_streams != 0`.\n",
    "\n",
    "const uint64_t num_entries = 10;\n",
    "const uint64_t num_iters = 1UL << 10;\n",
    "\n",
    "cudaMallocHost(&data_cpu, sizeof(uint64_t)*num_entries);\n",
    "cudaMalloc    (&data_gpu, sizeof(uint64_t)*num_entries);\n",
    "\n",
    "// Set the number of streams to not evenly divide num_entries.\n",
    "const uint64_t num_streams = 3;\n",
    "\n",
    "cudaStream_t streams[num_streams];\n",
    "for (uint64_t stream = 0; stream < num_streams; stream++)\n",
    "    cudaStreamCreate(&streams[stream]);\n",
    "\n",
    "// Use round-up division (`sdiv`, defined in helper.cu) so `num_streams*chunk_size`\n",
    "// is never less than `num_entries`.\n",
    "// This can result in `num_streams*chunk_size` being greater than `num_entries`, meaning\n",
    "// we will need to guard against out-of-range errors in the final \"tail\" stream (see below).\n",
    "const uint64_t chunk_size = sdiv(num_entries, num_streams);\n",
    "\n",
    "for (uint64_t stream = 0; stream < num_streams; stream++) {\n",
    "\n",
    "    const uint64_t lower = chunk_size*stream;\n",
    "    // For tail stream `lower+chunk_size` could be out of range, so here we guard against that.\n",
    "    const uint64_t upper = min(lower+chunk_size, num_entries);\n",
    "    // Since the tail stream width may not be `chunk_size`,\n",
    "    // we need to calculate a separate `width` value.\n",
    "    const uint64_t width = upper-lower;\n",
    "\n",
    "    // Use `width` instead of `chunk_size`.\n",
    "    cudaMemcpyAsync(data_gpu+lower, data_cpu+lower, \n",
    "           sizeof(uint64_t)*width, cudaMemcpyHostToDevice, \n",
    "           streams[stream]);\n",
    "\n",
    "    // Use `width` instead of `chunk_size`.\n",
    "    decrypt_gpu<<<80*32, 64, 0, streams[stream]>>>\n",
    "        (data_gpu+lower, width, num_iters);\n",
    "\n",
    "    // Use `width` instead of `chunk_size`.\n",
    "    cudaMemcpyAsync(data_cpu+lower, data_gpu+lower, \n",
    "           sizeof(uint64_t)*width, cudaMemcpyDeviceToHost, \n",
    "           streams[stream]);\n",
    "}\n",
    "\n",
    "// Destroy streams.\n",
    "for (uint64_t stream = 0; stream < num_streams; stream++)\n",
    "    cudaStreamDestroy(streams[stream]);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#65AE11;\">Check for Understanding</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please answer the following to confirm you've learned the main objectives of this section. You can display the answers for each question by clicking on the \"...\" cells below the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is problematic about using 3 separate non-default streams: one for all host-to-device memory transfer, one for all GPU compute, and one for all device-to-host memory transfer?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "There is no guaranteed ordering between operations in different non-default streams. Issuing host-to-device transfer, GPU compute, and device-to-host transfer each in their own non-default stream could violate the constraints that GPU compute depends on the completion of host-to-device transfer, and that device-to-host transfer depends on the completion of GPU compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why is partitioning data into chunks an effective component of acheiving copy/compute overlap?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "When we chunk our data we can...\n",
    "\n",
    "1. Maintain correct operational order between host-to-device transfer, GPU compute, and device-to-host transfer for each chunk of data by using the same non-default stream for each of these 3 operations\n",
    "2. Use different non-default streams for different chunks of data to create the possibility of overlapping copy in a given non-default stream with compute (and copy in the other direction) in another non-default stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#65AE11;\">Next</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equipped with an understanding of how to acheive copy/compute overlap for an arbitrary amount of data and number of streams, you will in the next section apply your understanding to acheive copy/compute overlap in the cipher application.\n",
    "\n",
    "Please continue to the next section: [*Exercise: Apply Copy/Compute Overlap*](../09_Exercise_Apply_Streams/Exercise_Apply_Streams.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#65AE11;\">Optional Further Study</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are for students with time and interest to do additional study on topics related to this workshop.\n",
    "\n",
    "* The GTC presentation [*CUDA Streams: Best Practices and Common Pitfalls*](https://on-demand.gputechconf.com/gtc/2014/presentations/S4158-cuda-streams-best-practices-common-pitfalls.pdf) will give you a review of many topics covered in this workshop, as well as some additional topics, and examples of common scenarios where the use of streams can go wrong."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
